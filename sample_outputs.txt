======================================================================
N-Gram Language Model - Texts Generation Results
Generated from Frankenstein by Mary Shelley (~78,000 words)
======================================================================

- Project: Statistical Language Modeling with Markov Chains
- Training corpus: Frankenstein by Mary Shelley (~78,000 words, 7,396 unique tokens, frankenstein.txt)
- Token count: 30 tokens per generated sample
- Random seed: 42 (for reproducibility)
- Models tested: Unigram, Bigram, Trigram, 4-gram

Test sentences for perplexity evaluation:
1. "I was alone."
2. "The monster appeared."
3. "She smiled."

======================================================================
PART 1: TEXT GENERATION RESULTS
======================================================================

1-gram Model (unigram)
----------------------------------------------------------------------
Generated text:
my , and The rain of to . emaciated , Safie heard , I no in Sometimes make than " talk on beloved <END> where before . . the me

Analysis:
❌ COHERENCE: None - completely random word sequences
❌ GRAMMAR: No grammatical structure observable
❌ READABILITY: 0% - unintelligible
✅ VOCABULARY: All words from source corpus (valid tokens)

Interpretation:
Unigram treats each word independently with no context. High perplexity (243.90) indicates poor predictive power. 


2-gram Model (bigram)
----------------------------------------------------------------------
Generated text:
My application . <END> Should she was a large . <END> I alighted and prepared myself ? <END> The God knows that crossed my various keys were altered to our

Analysis:
⚠️ COHERENCE: Local word pairs make sense ("I alighted and prepared myself")
⚠️ GRAMMAR: Basic structure emerging but fragmented
⚠️ READABILITY: ~30% - short phrases readable, overall broken
✅ PERPLEXITY: 95.8% improvement over unigram (243.90 → 10.23)
❌ SPARSITY: 67% of test sentences contain unseen bigrams

Interpretation:
Bigram captures adjacent word relationships. Phrases like "I alighted and"
demonstrate correct English syntax at the local level. However, frequent
<END> tokens indicate short sentences from training data. The model shows
dramatic perplexity reduction but suffers from data sparsity (∞ values).


3-gram Model (trigram)
----------------------------------------------------------------------
Generated text:
My ancestors had been so miserable as I gazed on him as I commence my task . <END> The God of heaven , and which I believed in her last

Analysis:
✅ COHERENCE: Strong - maintains narrative flow within sentences
✅ GRAMMAR: Correct complex sentence structures
✅ READABILITY: ~70% - most phrases make semantic sense
✅ PERPLEXITY: 25.6% improvement over bigram (10.23 → 7.61)
⚠️ REPETITION: Minor pattern repetition ("as I ... as I")
❌ SPARSITY: Still 67% unseen trigrams in test set

Interpretation:
Trigram produces grammatically correct, emotionally resonant prose
matching Frankenstein's Gothic style. Phrases like "My ancestors had
been so miserable" and "The God of heaven" demonstrate proper long-range
dependencies. Perplexity continues to decrease but sparsity remains.


4-gram Model
----------------------------------------------------------------------
Generated text:
My ancestors had been for many years , and they are sufficient to conquer all fear of ignominy or death . <END> Her garb was rustic , and her immutable

Analysis:
✅ COHERENCE: Excellent - near-human quality writing
✅ GRAMMAR: Complex subordinate clauses, perfect syntax
✅ READABILITY: ~90% - indistinguishable from original text
✅ VOCABULARY: Rich, contextually appropriate ("ignominy", "rustic")
✅ PERPLEXITY: 23.4% improvement over trigram (7.61 → 5.83)
⚠️ MEMORIZATION: Likely reproducing exact training sequences

Interpretation:
4-gram generates publication-quality prose with sophisticated vocabulary
and complex sentence structures. The phrase "sufficient to conquer all
fear of ignominy or death" demonstrates mastery of 19th-century literary
style. However, lowest perplexity (5.83) suggests possible overfitting-
model may be memorizing rather than generalizing. This is expected given
the small corpus size (78K words).


======================================================================

💡 Observation:
   As n increases, generated text becomes more coherent
   but may also become more repetitive (memorization).

======================================================================






======================================================================
PART 2: N-Gram Model Perplexity Comparison
======================================================================

Training models on frankenstein.txt...

1-gram Model Perplexity:
  'I was alone.': 85.46
  'The monster appeared.': 258.53
  'She smiled.': 387.72
  Average: 243.90

2-gram Model Perplexity:
  'I was alone.': 10.23
  'The monster appeared.': ∞ (unseen n-gram)
  'She smiled.': ∞ (unseen n-gram)
  Average: 10.23 (1/3 sentences with finite perplexity)

3-gram Model Perplexity:
  'I was alone.': 7.61
  'The monster appeared.': ∞ (unseen n-gram)
  'She smiled.': ∞ (unseen n-gram)
  Average: 7.61 (1/3 sentences with finite perplexity)

4-gram Model Perplexity:
  'I was alone.': 5.83
  'The monster appeared.': ∞ (unseen n-gram)
  'She smiled.': ∞ (unseen n-gram)
  Average: 5.83 (1/3 sentences with finite perplexity)
 
======================================================================
Summary: Average Perplexity (excluding ∞)
======================================================================

┌──────────┬──────────────┬───────────────┬─────────────────┬──────────────┐
│ Model    │ Avg Perp.    │ Improvement   │ Finite Values   │ Coherence    │
├──────────┼──────────────┼───────────────┼─────────────────┼──────────────┤
│ 1-gram   │ 243.90       │ baseline      │ 3/3 (100%)      │ None         │
│ 2-gram   │  10.23       │ 95.8% ↓       │ 1/3 (33%)       │ Local        │
│ 3-gram   │   7.61       │ 96.9% ↓       │ 1/3 (33%)       │ Strong       │
│ 4-gram   │   5.83       │ 97.6% ↓       │ 1/3 (33%)       │ Excellent    │
└──────────┴──────────────┴───────────────┴─────────────────┴──────────────┘






======================================================================
PART 3: Data Sparsity Analysis
======================================================================

Analyzing vocabulary coverage for each model...

1-gram Model Statistics:
  • Unique n-grams:     7,396
  • Unique contexts:        1
  • Sample n-grams:
      '∅' → 'Letter': 4 times
      '∅' → '1': 2 times
      '∅' → '<END>': 3374 times

2-gram Model Statistics:
  • Unique n-grams:    42,315
  • Unique contexts:    7,396
  • Sample n-grams:
      '<START>' → 'Letter': 4 times
      'Letter' → '1': 1 times
      '1' → '<END>': 2 times

3-gram Model Statistics:
  • Unique n-grams:    71,893
  • Unique contexts:   42,275
  • Sample n-grams:
      '<START> <START>' → 'Letter': 4 times
      '<START> Letter' → '1': 1 times
      'Letter 1' → '<END>': 1 times

4-gram Model Statistics:
  • Unique n-grams:    81,680
  • Unique contexts:   70,204
  • Sample n-grams:
      '<START> <START> <START>' → 'Letter': 4 times
      '<START> <START> Letter' → '1': 1 times
      '<START> Letter 1' → '<END>': 1 times

Vocabulary Coverage Statistics:
┌──────────┬─────────────────┬──────────────────┬─────────────────────┐
│ Model    │ Unique N-grams  │ Unique Contexts  │ Coverage Issue      │
├──────────┼─────────────────┼──────────────────┼─────────────────────┤
│ 1-gram   │       7,396     │              1   │ None                │
│ 2-gram   │      42,315     │          7,396   │ Emerging            │
│ 3-gram   │      71,893     │         42,275   │ Significant         │
│ 4-gram   │      81,680     │         70,204   │ Severe              │
└──────────┴─────────────────┴──────────────────┴─────────────────────┘

======================================================================
Key Insight:
======================================================================

As n increases, the number of possible n-grams grows exponentially:
  • Vocabulary Size (V) = 7,396 unique tokens
  • Possible bigrams:  V² = 54.7 million    → Observed: 42,315 (0.077%)
  • Possible trigrams: V³ = 404 billion     → Observed: 71,893 (0.000018%)
  • Possible 4-grams:  V⁴ = 3.0 trillion    → Observed: 81,680 (0.0000027%)

Interpretation:
  The exponential growth in possible n-grams vastly exceeds the linear
  growth of the training corpus. With only 78,000 words, we observe less
  than 0.01% of possible bigrams and a negligible fraction of higher-order
  n-grams. This is why 67% of test sentences produce infinite perplexity
  for n ≥ 2.
