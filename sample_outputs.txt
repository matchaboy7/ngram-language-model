======================================================================
N-Gram Language Model - Texts Generation Results
Generated from Frankenstein by Mary Shelley (~78,000 words)
======================================================================

- Project: Statistical Language Modeling with Markov Chains
- Training corpus: Frankenstein by Mary Shelley (~78,000 words, 7,396 unique tokens, frankenstein.txt)
- Token count: 30 tokens per generated sample
- Random seed: 42 (for reproducibility)
- Models tested: Unigram, Bigram, Trigram, 4-gram

Test sentences for perplexity evaluation:
1. "I was alone."
2. "The monster appeared."
3. "She smiled."

======================================================================
PART 1: TEXT GENERATION RESULTS
======================================================================

1-gram Model (unigram)
----------------------------------------------------------------------
Generated text:
my , and The rain of to . emaciated , Safie heard , I no in Sometimes make than " talk on beloved <END> where before . . the me

Analysis:
âŒ COHERENCE: None - completely random word sequences
âŒ GRAMMAR: No grammatical structure observable
âŒ READABILITY: 0% - unintelligible
âœ… VOCABULARY: All words from source corpus (valid tokens)

Interpretation:
Unigram treats each word independently with no context. High perplexity (243.90) indicates poor predictive power. 


2-gram Model (bigram)
----------------------------------------------------------------------
Generated text:
My application . <END> Should she was a large . <END> I alighted and prepared myself ? <END> The God knows that crossed my various keys were altered to our

Analysis:
âš ï¸ COHERENCE: Local word pairs make sense ("I alighted and prepared myself")
âš ï¸ GRAMMAR: Basic structure emerging but fragmented
âš ï¸ READABILITY: ~30% - short phrases readable, overall broken
âœ… PERPLEXITY: 95.8% improvement over unigram (243.90 â†’ 10.23)
âŒ SPARSITY: 67% of test sentences contain unseen bigrams

Interpretation:
Bigram captures adjacent word relationships. Phrases like "I alighted and"
demonstrate correct English syntax at the local level. However, frequent
<END> tokens indicate short sentences from training data. The model shows
dramatic perplexity reduction but suffers from data sparsity (âˆ values).


3-gram Model (trigram)
----------------------------------------------------------------------
Generated text:
My ancestors had been so miserable as I gazed on him as I commence my task . <END> The God of heaven , and which I believed in her last

Analysis:
âœ… COHERENCE: Strong - maintains narrative flow within sentences
âœ… GRAMMAR: Correct complex sentence structures
âœ… READABILITY: ~70% - most phrases make semantic sense
âœ… PERPLEXITY: 25.6% improvement over bigram (10.23 â†’ 7.61)
âš ï¸ REPETITION: Minor pattern repetition ("as I ... as I")
âŒ SPARSITY: Still 67% unseen trigrams in test set

Interpretation:
Trigram produces grammatically correct, emotionally resonant prose
matching Frankenstein's Gothic style. Phrases like "My ancestors had
been so miserable" and "The God of heaven" demonstrate proper long-range
dependencies. Perplexity continues to decrease but sparsity remains.


4-gram Model
----------------------------------------------------------------------
Generated text:
My ancestors had been for many years , and they are sufficient to conquer all fear of ignominy or death . <END> Her garb was rustic , and her immutable

Analysis:
âœ… COHERENCE: Excellent - near-human quality writing
âœ… GRAMMAR: Complex subordinate clauses, perfect syntax
âœ… READABILITY: ~90% - indistinguishable from original text
âœ… VOCABULARY: Rich, contextually appropriate ("ignominy", "rustic")
âœ… PERPLEXITY: 23.4% improvement over trigram (7.61 â†’ 5.83)
âš ï¸ MEMORIZATION: Likely reproducing exact training sequences

Interpretation:
4-gram generates publication-quality prose with sophisticated vocabulary
and complex sentence structures. The phrase "sufficient to conquer all
fear of ignominy or death" demonstrates mastery of 19th-century literary
style. However, lowest perplexity (5.83) suggests possible overfitting-
model may be memorizing rather than generalizing. This is expected given
the small corpus size (78K words).


======================================================================

ğŸ’¡ Observation:
   As n increases, generated text becomes more coherent
   but may also become more repetitive (memorization).

======================================================================






======================================================================
PART 2: N-Gram Model Perplexity Comparison
======================================================================

Training models on frankenstein.txt...

1-gram Model Perplexity:
  'I was alone.': 85.46
  'The monster appeared.': 258.53
  'She smiled.': 387.72
  Average: 243.90

2-gram Model Perplexity:
  'I was alone.': 10.23
  'The monster appeared.': âˆ (unseen n-gram)
  'She smiled.': âˆ (unseen n-gram)
  Average: 10.23 (1/3 sentences with finite perplexity)

3-gram Model Perplexity:
  'I was alone.': 7.61
  'The monster appeared.': âˆ (unseen n-gram)
  'She smiled.': âˆ (unseen n-gram)
  Average: 7.61 (1/3 sentences with finite perplexity)

4-gram Model Perplexity:
  'I was alone.': 5.83
  'The monster appeared.': âˆ (unseen n-gram)
  'She smiled.': âˆ (unseen n-gram)
  Average: 5.83 (1/3 sentences with finite perplexity)
 
======================================================================
Summary: Average Perplexity (excluding âˆ)
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model    â”‚ Avg Perp.    â”‚ Improvement   â”‚ Finite Values   â”‚ Coherence    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1-gram   â”‚ 243.90       â”‚ baseline      â”‚ 3/3 (100%)      â”‚ None         â”‚
â”‚ 2-gram   â”‚  10.23       â”‚ 95.8% â†“       â”‚ 1/3 (33%)       â”‚ Local        â”‚
â”‚ 3-gram   â”‚   7.61       â”‚ 96.9% â†“       â”‚ 1/3 (33%)       â”‚ Strong       â”‚
â”‚ 4-gram   â”‚   5.83       â”‚ 97.6% â†“       â”‚ 1/3 (33%)       â”‚ Excellent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜






======================================================================
PART 3: Data Sparsity Analysis
======================================================================

Analyzing vocabulary coverage for each model...

1-gram Model Statistics:
  â€¢ Unique n-grams:     7,396
  â€¢ Unique contexts:        1
  â€¢ Sample n-grams:
      'âˆ…' â†’ 'Letter': 4 times
      'âˆ…' â†’ '1': 2 times
      'âˆ…' â†’ '<END>': 3374 times

2-gram Model Statistics:
  â€¢ Unique n-grams:    42,315
  â€¢ Unique contexts:    7,396
  â€¢ Sample n-grams:
      '<START>' â†’ 'Letter': 4 times
      'Letter' â†’ '1': 1 times
      '1' â†’ '<END>': 2 times

3-gram Model Statistics:
  â€¢ Unique n-grams:    71,893
  â€¢ Unique contexts:   42,275
  â€¢ Sample n-grams:
      '<START> <START>' â†’ 'Letter': 4 times
      '<START> Letter' â†’ '1': 1 times
      'Letter 1' â†’ '<END>': 1 times

4-gram Model Statistics:
  â€¢ Unique n-grams:    81,680
  â€¢ Unique contexts:   70,204
  â€¢ Sample n-grams:
      '<START> <START> <START>' â†’ 'Letter': 4 times
      '<START> <START> Letter' â†’ '1': 1 times
      '<START> Letter 1' â†’ '<END>': 1 times

Vocabulary Coverage Statistics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model    â”‚ Unique N-grams  â”‚ Unique Contexts  â”‚ Coverage Issue      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1-gram   â”‚       7,396     â”‚              1   â”‚ None                â”‚
â”‚ 2-gram   â”‚      42,315     â”‚          7,396   â”‚ Emerging            â”‚
â”‚ 3-gram   â”‚      71,893     â”‚         42,275   â”‚ Significant         â”‚
â”‚ 4-gram   â”‚      81,680     â”‚         70,204   â”‚ Severe              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

======================================================================
Key Insight:
======================================================================

As n increases, the number of possible n-grams grows exponentially:
  â€¢ Vocabulary Size (V) = 7,396 unique tokens
  â€¢ Possible bigrams:  VÂ² = 54.7 million    â†’ Observed: 42,315 (0.077%)
  â€¢ Possible trigrams: VÂ³ = 404 billion     â†’ Observed: 71,893 (0.000018%)
  â€¢ Possible 4-grams:  Vâ´ = 3.0 trillion    â†’ Observed: 81,680 (0.0000027%)

Interpretation:
  The exponential growth in possible n-grams vastly exceeds the linear
  growth of the training corpus. With only 78,000 words, we observe less
  than 0.01% of possible bigrams and a negligible fraction of higher-order
  n-grams. This is why 67% of test sentences produce infinite perplexity
  for n â‰¥ 2.
